{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubic - Kubernetes Infrastructure as Code","text":"<p>Available on:</p> <p> </p>"},{"location":"#overview","title":"Overview","text":"<p>Kubic is a cutting edge, ready for production and multi cloud provider Kubernetes infrastructure as code. It integates an ingress controller, a certificate manager, a monitoring stack, a GitOps tool with complete secret management and a backup tool.</p> <p>This Terraform aims at creating a managed k8s cluster setup with :</p> <ul> <li>NGINX Ingress Controller</li> <li>Cert-manager</li> <li>Prometheus / Grafana</li> <li>ArgoCD</li> <li>Hashicorp Vault if needed</li> <li>ArgoCD Vault Plugin if Vault is deployed</li> <li>Velero for backuping the cluster</li> <li>Loki if enabled</li> </ul> <p>The cluster can be deployed either on OVHCloud or on Scaleway. New provider can be added by creating a new folder in the root of the repository, and by following the same architecture as the existing providers.</p>"},{"location":"#repository-architecture","title":"Repository architecture","text":"<pre><code>.\n\u251c\u2500\u2500 docs                  # Folder containing the documentation\n\u251c\u2500\u2500 state_bucket          # Folder containing the Terraform to create a S3 bucket for the Terraform state\n\u251c\u2500\u2500 vault                 # Folder containing the Terraform to configure Hashicorp Vault\n\u251c\u2500\u2500 common                # Folder containing the Terraform which is common to all the providers\n\u251c\u2500\u2500 ovh                   # Folder declaring Terraform to deploy a cluster on OVHCloud\n\u251c\u2500\u2500 scaleway              # Folder declaring Terraform to deploy a cluster on Scaleway\n\u251c\u2500\u2500 examples              # Folder containing examples of applications to deploy with ArgoCD\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 LICENSE\n\u2514\u2500\u2500 README.md\n</code></pre> <p>All files contained in the folder <code>common</code> are symbolicaly linked in the folders <code>ovh</code> and <code>scaleway</code> to avoid code duplication.</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Create you cluster:</li> <li>Manual deployment</li> <li>Automatic deployment</li> <li>Configure Hashicorp Vault</li> <li>Configure ArgoCD</li> <li>Configure Velero</li> <li>Standalone use</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Currently, only OVH and Scaleway are supported as providers. Here are the guidelines to add a new provider:</p> <ul> <li>Create a new folder in the root of the repository, with the name of the provider;</li> <li>Create a symlink for all files in <code>common</code> to your new folder;</li> <li>Create a <code>terraform.tf</code> file containing:</li> <li>Terraform configuration with a <code>s3</code> backend;</li> <li>The <code>helm</code>, <code>kubernetes</code> and <code>kubectl</code> providers along with the provider(s) you need, correctly configured;</li> <li>A <code>kubernetes.tf</code> file creating the cluster, with an output named <code>kubeconfig</code> that contains the actual kubeconfig for the cluster;</li> <li>A <code>ingress-nginx.tf</code> file, deploying the ingress-nginx ingress controller and configuring it with an external IP (you may need to create a load balancer on your provider). The ingress IP should be a Terraform output named <code>ingress_ip</code>;</li> <li>This must also create a <code>null_resource</code> named <code>ingress-nginx</code> that will <code>depends_on</code> on the node pool of your cluster (this is to get a consistent dependency chain for Terraform)</li> <li>The controller must have at least the following configuration:</li> </ul> <pre><code>controller:\n  metrics:\n    enabled: true\n    serviceMonitor:\n      additionalLabels:\n        release: prometheus\n      enabled: true\n  extraArgs:\n    enable-ssl-passthrough: true\n  admissionWebhooks:\n    timeoutSeconds: 30\n</code></pre> <ul> <li>Edit the <code>docker-compose.yaml</code> and create a service (adapt merely the code) for your provider.</li> </ul>"},{"location":"argocd/","title":"ArgoCD","text":"<p>Before reading this section, please note that disabling the installation of Hashicorp Vault will also disable the installation of ArgoCD Vault Plugin. You are still able to use ArgoCD the way you want but you will have to use your own repo structure.</p>"},{"location":"argocd/#the-mono-repo","title":"The mono-repo","text":"<p>The mono-repo is a git repository containing all the applications you want to deploy on your cluster. It is used by ArgoCD to deploy your applications. It is a good practice to have a mono-repo for each cluster you have.</p> <p>This project shares a mono-repo structure which was specifically designed to ease the deployment of applications for new k8s users. It is available here. However, you may be free to use your own repository structure.</p> <p>The repostiory structure is the following :</p> <pre><code>.\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 apps                                # Folder containing all the applications to declare\n\u2502   \u251c\u2500\u2500 external-app                    # Folder declaring the external-app application\n\u2502   \u2502   \u2514\u2500\u2500 test.json\n\u2502   \u251c\u2500\u2500 hello-world                     # Folder declaring the hello-world application\n\u2502   \u2502   \u251c\u2500\u2500 preprod.json\n\u2502   \u2502   \u251c\u2500\u2500 prod.json\n\u2502   \u2502   \u2514\u2500\u2500 staging.json\n\u2502   \u2514\u2500\u2500 secret-helm                     # Folder declaring the secret-helm application\n\u2502       \u251c\u2500\u2500 base.yaml\n\u2502       \u251c\u2500\u2500 dev.json\n\u2502       \u251c\u2500\u2500 dev.yaml\n\u2502       \u251c\u2500\u2500 prod.json\n\u2502       \u2514\u2500\u2500 prod.yaml\n\u2514\u2500\u2500 helm                                # Folder containing all the helm charts\n    \u251c\u2500\u2500 hello-world                     # Folder containing the hello-world helm chart\n    \u2502   \u251c\u2500\u2500 .helmignore\n    \u2502   \u251c\u2500\u2500 Chart.yaml\n    \u2502   \u251c\u2500\u2500 README.md\n    \u2502   \u251c\u2500\u2500 templates\n    \u2502   \u2502   \u251c\u2500\u2500 NOTES.txt\n    \u2502   \u2502   \u251c\u2500\u2500 _helpers.tpl\n    \u2502   \u2502   \u251c\u2500\u2500 deployment.yaml\n    \u2502   \u2502   \u251c\u2500\u2500 service.yaml\n    \u2502   \u2502   \u2514\u2500\u2500 serviceaccount.yaml\n    \u2502   \u2514\u2500\u2500 values.yaml\n    \u2514\u2500\u2500 secret-helm                     # Folder containing the secret-helm helm chart\n        \u251c\u2500\u2500 .DS_Store\n        \u251c\u2500\u2500 .helmignore\n        \u251c\u2500\u2500 Chart.yaml\n        \u251c\u2500\u2500 templates\n        \u2502   \u251c\u2500\u2500 .DS_Store\n        \u2502   \u2514\u2500\u2500 secret.yaml\n        \u2514\u2500\u2500 values.yaml\n</code></pre>"},{"location":"argocd/#usage","title":"Usage","text":"<ol> <li>Create a new repository with the same structure as the mono-repo</li> <li>Create read credentials for the repository (see here for different providers)</li> <li>Configure accordingly the Terraform variables <code>argocd_repo_url</code>, <code>argocd_repo_username</code> and <code>argocd_repo_password</code> (see variables.tf). Terraform expects HTTP git credentials, not SSH.</li> <li>Define the variables <code>argocd_hostname</code> and <code>argocd_password</code> (see variables.tf). The variable <code>argocd_password</code> is used to define the password of the <code>admin</code> user of ArgoCD. Terraform expects a hash of the password. To generate it, you can use the following command : <code>argocd account bcrypt --password P@$sw0rd</code> after installing ArgoCD CLI.</li> </ol>"},{"location":"argocd/#argocd-vault-plugin","title":"ArgoCD Vault Plugin","text":"<p>The ArgoCD Vault Plugin is a plugin for ArgoCD which allows to use secrets stored in Hashicorp Vault in your applications. It is installed by default on the cluster. You can fine tune its version by changing the variable <code>argocd_avp_version</code> (see variables.tf). It is highly recommended to read the documentation of the plugin before using it as it has many undocumented features in this README that may suit your needs.</p> <p>By default, ArgoCD Vault Plugin is configured to use the Kubernetes auth backend of Vault. The authentication is done with the Kubernetes service account of ArgoCD in the <code>argocd</code> namespace. The service account has read access on the path <code>kv/*</code>. We'll see later how to restrict the access to the secrets for specific applications.</p> <p>ArgoCD Vault Plugin works by taking a directory of YAML files that have been templated out using the pattern of <code>&lt;placeholder&gt;</code> and then using the values from Vault to replace the placeholders. The plugin will then apply the YAML files to the cluster. You can use generic or inline placeholders. However, inline placeholders are more straightforward to use. An inline-path placeholder allows you to specify the path, key, and optionally, the version to use for a specific placeholder. This means you can inject values from multiple distinct secrets in your secrets manager into the same YAML.</p> <p>Valid examples:</p> <pre><code>- &lt;path:some/path#secret-key&gt;\n- &lt;path:some/path#secret-key#version&gt;\n</code></pre> <p>If the version is omitted (first example), the latest version of the secret is retrieved. By default, Vault creates a KV-V2 backend. For KV-V2 backends, the path needs to be specified as <code>&lt;path:${vault-kvv2-backend-path}/data/{path-to-secret}&gt;</code> where <code>vault-kvv2-backend-path</code> is the path to the KV-V2 backend and <code>path-to-secret</code> is the path to the secret in Vault.</p> <p>Again, it is highly recommended to read the placeholders documentation of the plugin before using it.</p>"},{"location":"argocd/#examples","title":"Examples","text":""},{"location":"argocd/#basic-example-hello-world-application","title":"Basic example - hello-world application","text":"<p>This example shows how to deploy a simple application with ArgoCD. The application is a simple nginx server. The application is deployed in 3 environments: staging, preprod and prod. The application is deployed in 3 different namespaces, one namespace per application and per environment.</p> <p>The application is deployed with the following instructions :</p> <ul> <li>Add the <code>hello-world</code> helm chart to the <code>helm</code> folder of the mono-repo</li> <li>Declare the application in the <code>apps</code> folder of the mono-repo by creating a folder named <code>hello-world</code>. Beware of the name of the folder, it must be the same as the name of the helm chart.</li> <li>Add a JSON file that fits your needs:</li> <li>Add a JSON file per environment and name the file according to the following pattern: <code>&lt;environment&gt;.json</code>. For instance, for the staging environment, the file must be named <code>staging.json</code>.</li> <li>Add a JSON file named after the application folder. For instance <code>hello-world.json</code>. This will create a standalone application without any environment.</li> </ul> <p>This file must be a valid JSON file and must contain at least:</p> <pre><code>{}\n</code></pre>"},{"location":"argocd/#argocd-vault-plugin-example-secret-helm-application","title":"ArgoCD Vault Plugin example - secret-helm application","text":"<p>This example shows how to use the ArgoCD Vault Plugin to deploy a helm chart with secrets stored in Hashicorp Vault. The application is a simple chart which creates a secret with with various keys and values. The application is deployed in 2 environments: dev and prod. The application is deployed in 2 different namespaces, one namespace per application and per environment.</p> <p>The application configuration refers to specific helm values per environment. The used value files are declared for each environment using the JSON file. The JSON file must contain the following:</p> <pre><code>{\n  \"valuesFiles\": [\"&lt;path-to-values-file&gt;\"]\n}\n</code></pre> <p>For instance, the prod environment uses the <code>prod.json</code> file with:</p> <pre><code>{\n  \"valuesFiles\": [\"base.yaml\", \"prod.yaml\"]\n}\n</code></pre>"},{"location":"argocd/#multi-tenancy-example-external-app-application","title":"Multi-tenancy example - external-app application","text":"<p>This example shows how to deploy an application in a multi-tenant environment. The cluster administrator is responsible for declaring the application on the cluster and the developers are responsible for maintaining the application helm chart. This is achieved by specifying the <code>externalRepoURL</code> in the JSON file.</p> <p>For instance, the test environment uses the <code>test.json</code> file with:</p> <pre><code>{\n  \"externalRepoURL\": \"https://github.com/example/externalRepo.git\"\n}\n</code></pre> <p>Beware the distant repository must be public or the cluster must have access to it. Please refer to the ArgoCD documentation for more information.</p> <p>Please also note that the distant repository must have the exact same structure as the mono-repo. The distant repository must contain a <code>helm</code> folder with the helm charts and an <code>apps</code> folder with the application configuration:</p> <pre><code>.\n\u251c\u2500\u2500 apps\n\u2502   \u2514\u2500\u2500 external-app\n\u2502       \u2514\u2500\u2500 test.yaml\n\u2514\u2500\u2500 helm\n    \u2514\u2500\u2500 external-app\n        \u251c\u2500\u2500 .helmignore\n        \u251c\u2500\u2500 Chart.yaml\n        \u251c\u2500\u2500 charts\n        \u251c\u2500\u2500 templates\n        \u2514\u2500\u2500 values.yaml\n</code></pre> <p>Just like that, the developer who controls the helm chart is able to request any secret contained in the vault just by using the correct path of a secret in the vault. Therefore, the cluster administrator must restrict the access to the secrets for specific applications. This is achieved by following this procedure :</p> <ol> <li>Create a specific policy in Vault for the application which only gives access to the secrets needed by the application</li> </ol> <p>RW policy example :</p> <pre><code>path \"kv/metadata/my_app/*\" {\n  capabilities = [\"list\", \"read\", \"delete\"]\n}\npath \"kv/data/my_app/*\" {\n  capabilities = [\"create\", \"update\", \"read\", \"delete\"]\n}\npath \"kv/delete/my_app/*\" {\n  capabilities = [\"update\"]\n}\npath \"kv/undelete/my_app/*\" {\n  capabilities = [\"update\"]\n}\npath \"kv/destroy/my_app/*\" {\n  capabilities = [\"update\"]\n}\n</code></pre> <ol> <li>Attach the policy to the Vault Authentication Method</li> <li>Create an ArgoCD Vault Plugin configuration secret which uses the Vault Authentication Method. Please refer to the ArgoCD Vault Plugin backend documentation and the ArgoCD Vault Plugin configuration documentation for more information. Here is an example of a configuration secret for AppRole authentication:</li> </ol> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: external-vault-credentials\n  namespace: argocd\ntype: Opaque\nstringData:\n  VAULT_ADDR: Your HashiCorp Vault Address\n  AVP_TYPE: vault\n  AVP_AUTH_TYPE: approle\n  AVP_ROLE_ID: Your AppRole Role ID\n  AVP_SECRET_ID: Your AppRole Secret ID\n</code></pre> <p>Beware, the secret must be created in the <code>argocd</code> namespace.</p> <ol> <li>Finally, reference the vault credentials secret in the JSON file:</li> </ol> <pre><code>{\n  \"vaultCredentials\": \"external-vault-credentials\"\n}\n</code></pre> <p>Please note that if you do not want to use external repositories, you can still declare a helm chart in the mono-repo which calls an external chart which has to be stored on a helm repository.</p> <p>Next step \u2192 Use Velero</p>"},{"location":"cluster-auto/","title":"[AUTOMATIC] Deployment steps","text":"<p>Every command must be run at the root of the repository</p>"},{"location":"cluster-auto/#prerequisites","title":"Prerequisites","text":"<p>Docker and docker-compose must be installed on your computer.</p>"},{"location":"cluster-auto/#create-terraforms-state","title":"Create Terraform's state","text":"<p>First, we need a s3 bucket to store the Terraform's state, so that it can be available everywhere (and not only on your computer). If you already have a bucket, you can skip this step.</p> <p>This repository provides a Terraform to create a bucket on OVH, but not on Scaleway. For this step, you will need OVH API credentials (<code>application key</code>, <code>secret key</code> and <code>consumer key</code>, as well as the project id in which you will create the bucket, see here to generate a token). You must add the following rights and replace {serviceName} by your OVH's Public Cloud project id :</p> <ul> <li>GET /cloud/project/{serviceName}/*</li> <li>PUT /cloud/project/{serviceName}/*</li> <li>POST /cloud/project/{serviceName}/*</li> <li>DELETE /cloud/project/{serviceName}/*</li> </ul> <p>Then execute the corresponding script : <code>bin/init-bucket.sh</code>, after entering all the required information, it will create a bucket on OVH;</p> <p>And finally, save the provided credentials <code>access_key</code>, <code>secret_key</code> and <code>bucket_name</code>, you will need them for the next step.</p>"},{"location":"cluster-auto/#create-and-provision-the-cluster","title":"Create and provision the cluster","text":""},{"location":"cluster-auto/#configure-the-backend","title":"Configure the backend","text":"<p>Now we've got our s3 bucket, we have to setup Terraform's backend, where it stores its state. For this, we will use the s3 bucket we just created (or the one you already have).</p> <p>Run <code>bin/bootstrap-backend.sh &lt;your provider&gt;</code> to create the backend. It will create a <code>backend.conf</code> file, which will be used by Terraform to store its state in the s3 bucket. Replace <code>&lt;your provider&gt;</code> either with <code>ovh</code> or <code>scaleway</code>.</p> <p>If you used the previous script to generate the bucket, here are some information you need:</p> <ul> <li>Region: <code>gra</code></li> <li>Endpoint: <code>https://s3.gra.io.cloud.ovh.net/</code></li> <li>Skip region validation: <code>true</code></li> <li>Skip credentials validation: <code>true</code></li> </ul>"},{"location":"cluster-auto/#provide-the-correct-information","title":"Provide the correct information","text":"<p>Terraform needs a few variables to create your cluster, please run <code>bin/bootstrap.sh &lt;your-provider&gt;</code> and provide the desired values for each parameter. You will need:</p> <ul> <li>The hostname for several services: ArgoCD, Grafana, Vault (if installed)</li> <li>A already existing S3 bucket for Velero (you can use the state_bucket terraform script to create a S3 bucket for Velero)</li> <li>ArgoCD needs a Git repository with HTTPS credentials for access. You can use a private repository, or a public one. If you use a private repository, you will need to provide the HTTPS credentials (username and password). If you use a public repository, you can leave the username and password empty.</li> <li>API keys for your provider:</li> <li>For OVH, see here</li> <li>For Scaleway, see here</li> </ul> <p>The script will prompt for the most common variables. By default, some variables are not prompted (and their default value is then used). If you wish, you can look into the <code>variables.tf</code> and the <code>variables-common.tf</code> files to see all the variables that can be set. Simply add them to the <code>terraform.tfvars</code> file.</p>"},{"location":"cluster-auto/#deploy-the-cluster","title":"Deploy the cluster","text":"<p>After your <code>terraform.tfvars</code> file has been successfully created, you can now deploy the cluster. Run <code>bin/terraform-init.sh &lt;your provider&gt;</code> to initialize Terraform. After this, run <code>bin/terraform-plan.sh &lt;your provider&gt;</code>, the output shows you what Terraform will do. If you are satisfied with the plan, run <code>bin/terraform-apply.sh &lt;your provider&gt;</code> to deploy the cluster. (Please ignore the output of the command beginning with 'To perform exactly these actions...')</p> <p>While running, the <code>terraform-apply.sh</code> script may crash (especially with OVH). If so, analyze the error. If it is related to timeouts or server errors, simply re-run the script. (if you encounter errors re-running <code>terraform-apply.sh</code>, try running <code>terraform-plan.sh</code> before). The script may last more than 10 minutes, please be patient.</p> <p>Warning: If the script were to crash, make sure Terraform has not been creating ressources (e.g. a k8s cluster) in the background (which has not been linked to the state due to the crash). If so, you will have to delete them manually.</p> <p>At the end of the script, please make the needed changes on your DNS (adding the ingress IP to the needed domains), you may be able to retrieve your Kubeconfig file with the following command: <code>bin/get-kube-config.sh &lt;your provider&gt;</code>.</p>"},{"location":"cluster-auto/#destroy-the-cluster","title":"Destroy the cluster","text":"<p>With: <code>bin/terraform-destroy &lt;your provider&gt;</code>. Warning: there is no confirmation, it will destroy the cluster immediately.</p> <p>Next step \u2192 Configure Hashicorp Vault</p>"},{"location":"cluster-manual/","title":"[MANUAL] Deployment steps","text":"<p>This assumes you have the Terraform CLI installed on your computer.</p>"},{"location":"cluster-manual/#create-terraforms-state","title":"Create Terraform's state","text":"<p>First, we need a s3 bucket to store the Terraform's state, so that it can be available everywhere (and not only on your computer). If you already have a bucket, you can skip this step.</p> <p>This repository provides a Terraform to create a bucket on OVH. For this step, you will need OVH API credentials (<code>application key</code>, <code>secret key</code> and <code>consumer key</code>, as well as the project id in which you will create the bucket, see here if you do not know how to get them).</p> <ul> <li>Go to <code>/state_bucket</code>, and do a <code>terraform init</code></li> <li>Copy the <code>terraform.tfvars.template</code> into <code>terraform.tfvars</code> and provide the correct variables in it. (description of the vars is available in the <code>variables.tf</code> file)</li> <li>At this step, we need to do a tiny trick coming from OVH :</li> </ul> <p>If you have AWS CLI already configured, you are good to go !</p> <p>Else, due to a limitation in Terraform dependency graph for providers initialization (see this long lasting issue) it is required to have the following environement variables defined (even if they are dummy one and overridden during the script execution) : AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY</p> <p>If they are not already defined you can use the following:</p> <pre><code>export AWS_ACCESS_KEY_ID=\"no_need_to_define_an_access_key\"\nexport AWS_SECRET_ACCESS_KEY=\"no_need_to_define_a_secret_key\"\n</code></pre> <ul> <li>Then create the bucket with a <code>terraform plan</code> followed by a <code>terraform apply</code></li> <li>Save the provided <code>access_key</code> et <code>secret_key</code> (because the <code>secret_key</code> is a secret, you need to use the <code>terraform output secret_key</code> command to get it)</li> </ul>"},{"location":"cluster-manual/#create-and-provision-the-cluster","title":"Create and provision the cluster","text":"<p>Put yourself in the folder corresponding to the provider you want</p> <p>Now we've got our s3 bucket, copy the <code>backend.conf.template</code> in a <code>backend.conf</code> file and fill it with the information you previously obtained. You may choose a name for your state file (using the <code>key</code> field). They are needed for Terraform to know in what state your cluster is or will be or has been.</p> <p>Next :</p> <ul> <li>Provide the correct variables in a <code>terraform.tfvars</code> file. List of variables is available in the <code>variables.tf</code> file and in the <code>variables-common.tf</code> file, along with description and default values;</li> <li>For Hashicorp Vault: if you do not have a custom certificate, just leave the following variables empty: <code>vault_api_signed_certificate</code>, <code>vault_api_private_key</code>, <code>vault_api_ca_bundle</code>.</li> <li>You will need API credentials for the provider you choose:<ul> <li>For OVH, see here</li> <li>For Scaleway, see here</li> </ul> </li> <li>Do a <code>terraform init -backend-config=backend.conf</code>, then <code>terraform plan</code> then <code>terraform apply</code> to create your cluster. Doing so, your Terraform state will be saved in the s3 bucket.</li> </ul> <p>Using the OVH provider, you may encounter timeouts, or other errors. (coming from OVH) If so, simply re-run the <code>terraform apply</code> command. It will continue where it stopped and will eventually complete.</p> <p>Next step \u2192 Scripted cluster creation</p>"},{"location":"hashicorp-vault/","title":"Hashicorp Vault","text":"<p>Once the cluster has been setup, Hashicorp Vault (now referred to as \"vault\") is not ready for use. It has to be initialized and to be unsealed. Secrets will be handled in the following steps. To ensure HA on the cluster, the deployment consists of 3 pods, spread on 3 nodes. (the node autoscaling feature is used here). More pods can be created by modifying Terraform's vars. (HPA is not available though).</p> <p>To perform the following steps, you need to have every pod in the <code>Running</code> state. You can check this with <code>kubectl get pods -n hashicorp-vault</code>. (they won't be marked as ready however)</p>"},{"location":"hashicorp-vault/#manual-initialization","title":"Manual initialization","text":"<p>Initialization of the vault</p> <p>Shamir's algorithm is used to encrypt the vault. n keys (with n &gt; 0) are generated, and m keys (with 0 &lt; m &lt;= n) are needed to unseal the vault. This is achieved with the following command (using <code>kubectl</code> in the <code>hashicorp-vault</code> namespace):</p> <pre><code>kubectl exec hashicorp-vault-0 -- vault operator init \\\n    -key-shares=n \\\n    -key-threshold=m \\\n    -format=json &gt; cluster-keys.json\n</code></pre> <p>This command generates a <code>cluster-keys.json</code> file containing :</p> <ul> <li>the n generated keys</li> <li>a root token, used to authenticate to the vault (once unsealed)</li> </ul> <p>If you read the doc, you might want to make the pods join the Raft cluster. The vault is here configured to join the Raft cluster by itself, so no action is required from the user here.</p> <p>Unsealing of the vault</p> <p>The vault is still not available. Each pod must be unsealed to be operational. This can be achieved by doing so (still in the <code>hashicorp-vault</code> namespace), here with n = m = 1 :</p> <p><code>kubectl exec hashicorp-vault-i -- vault operator unseal $VAULT_UNSEAL_KEY</code>, with i going from 0 to the number of pods.</p> <p>Now, your vault is fully operational. First authentication is possible with the root token. The vault has to been unsealed everytime a pod is destroyed, or for any other reasons detailed in Hashicorp Vault's documentation.</p>"},{"location":"hashicorp-vault/#automatic-configuration","title":"Automatic configuration","text":"<p>The Vault may be automatically initialized and unsealed. This is done by executing the script <code>init.sh</code> in the <code>vault</code> folder, with the following command : <code>./init.sh</code>. Then follow the instructions and your Vault should be ready to use at the end.</p> <p>Initial configuration</p> <p>This part is not mandatory. It deploys the Key/Value engine on the Vault, as well as a Kubernetes backend for authentication (for instance used by the argocd-vault plugin). The k8s backend has read-access on the path <code>kv/*</code>.</p> <p>Go to the <code>vault</code> folder, copy <code>terraform.tfvars.template</code> to <code>terraform.tfvars</code> and fill it with the required variables. (description may be found in the <code>variables.tf</code> file). The <code>vault_root_token</code> may be found in the previously generated <code>cluster-keys.json</code> file. Then do a <code>terraform init</code>, followed by <code>terraform plan</code>, then <code>terraform apply</code>.</p> <p>Congratulations! Your Hashicorp Vault is now ready to use, enjoy!</p> <p>Next step \u2192 Configure ArgoCD</p>"},{"location":"standalone/","title":"Standalone use","text":"<p>If you already have a cluster (with another provider or whatever), you can still use this Terraform to deploy all the mentioned tools on it, if and only if you use ingress-nginx as your ingress controller. For this, you will need a <code>kubeconfig</code> file to access your cluster. (or your credentials, if so, you will have to modify by yourself the <code>terraform.tf</code> file).</p> <p>This part will install :</p> <ul> <li>Cert-manager, provisionned with issuers</li> <li>Prometheus along Grafana</li> <li>ArgoCD provisionned with a default repository</li> <li>Velero</li> <li>Hashicorp Vault</li> <li>Loki</li> </ul>"},{"location":"standalone/#requirements","title":"Requirements","text":"<p>Ingress-nginx must be configured with some values:</p> <pre><code>controller:\n  metrics:\n    enabled: true\n    serviceMonitor:\n      additionalLabels:\n        release: prometheus\n      enabled: true\n  extraArgs:\n    enable-ssl-passthrough: true\n</code></pre>"},{"location":"standalone/#steps","title":"Steps","text":"<p>Follow the following steps (every command must be run at the root of the repository):</p> <ul> <li>Run <code>bin/bootstrap.sh standalone</code> and fill the asked variables;</li> <li>Only the most common variables are prompted, if you want to change other variables, you will have to edit the <code>standalone/terraform.tfvars</code> file by yourself. (the complete list of variables is available in the <code>standalone/variables.tf</code> file)</li> <li>Run <code>bin/terraform-init.sh standalone</code> to initialize the Terraform state;</li> <li>Put your <code>kubeconfig</code> file in the <code>standalone</code> folder;</li> <li>Run <code>bin/terraform-plan.sh standalone</code> to see what will be deployed;</li> <li>Run <code>bin/terraform-apply.sh standalone</code> to deploy.</li> </ul>"},{"location":"velero/","title":"Velero","text":"<p>Velero is a backup and restore tool for Kubernetes. It is used to backup the cluster's resources, and to restore them in case of disaster. It is also used to migrate the cluster to another provider.</p> <p>Before using Velero, all you need is an external S3 bucket to store your backups. You can use any S3 compatible storage provider.  When creating the cluster, all velero variables are to be set either manualy in the <code>terraform.tfvars</code> file or during the automatic setup, according to the type of installation you choose.</p>"},{"location":"velero/#configuration","title":"Configuration","text":"<p>Set persistent volumes backup</p> <p>We use the opt-in approach from Velero to backup persistent volumes (more information here). This means that you need to add the following annotation to your pods, when you want its PVC to be saved : <code>backup.velero.io/backup-volumes: &lt;volumes_names&gt;, ...</code>. This will backup the persistent volume claim and the persistent volume associated with it.</p> <p>You can use the opt-out approach by setting <code>velero_default_volumes_to_fs_backup</code> to <code>true</code> in the <code>terraform.tfvars</code>.</p>"},{"location":"velero/#veleros-cli","title":"Velero's CLI","text":"<p>Velero comes with a CLI to manage the backups. You can install it here. To bind the CLI to your cluster, just set the <code>--kubeconfig</code> flag when you run a command. Otherwise, Velero will use your default kubeconfig file.</p> <p>Manual backup</p> <p>To backup the cluster, you need to create a backup file. This is done with the following command : <code>velero backup create BACKUP_NAME</code>. You can list your backups with <code>velero backup get</code>.</p> <p>Auto backup</p> <p>To schedule a backup of your namespace, just refer to the template <code>common/Schedlule-template.yaml.template</code> and fill it with the correct values. Then apply it with <code>kubectl apply -f Schedlule-template.yaml</code>.</p> <p>Restore from backup</p> <p>To restore from a backup, run the following command, with BACKUP_NAME being the name of the backup you want to restore from : <code>velero restore create --from-backup BACKUP_NAME</code>.</p> <p>Next step \u2192 Standalone use</p>"}]}